{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s4dk3KXiQyET",
        "outputId": "ce4adc0a-09ae-4756-ede3-a18cebf290bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset is downloaded\n",
            "squad1.1/train-v1.1.json\n",
            "Json file succesfully loaded\n",
            "Enter prepare csv file\n",
            "442\n",
            "442\n",
            "442\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "import sys\n",
        "import urllib.request\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from yaml import load\n",
        "\n",
        "class Preprocessing:\n",
        "    def __init__(self):\n",
        "        self.filename = 'train-v1.1.json'\n",
        "        self.directory = 'squad1.1'\n",
        "        self.url = \"https://rajpurkar.github.io/SQuAD-explorer/dataset/\"\n",
        "    \n",
        "    def start(self):\n",
        "        # if dataset doesnt exit then download it from the github repo\n",
        "        self.download_dataset(self.url, self.filename , self.directory)\n",
        "        print(\"Dataset is downloaded\")\n",
        "\n",
        "        # load dataset into a file\n",
        "        train_data = self.load_data(self.filename, self.directory)\n",
        "        print(\"Json file succesfully loaded\")\n",
        "\n",
        "        # prepare a csv file\n",
        "        self.prepare_csvfile(train_data)\n",
        "    \n",
        "    def prepare_csvfile(self, train_data):\n",
        "        context_list = []\n",
        "        question_list = []\n",
        "        answer_text_list = []\n",
        "        answer_start = []\n",
        "        context_data = []\n",
        "        question_data = []\n",
        "        answer_start_data = []\n",
        "        answer_length_data = []\n",
        "\n",
        "        print(\"Enter prepare csv file\")\n",
        "        for id in range(len(train_data[\"data\"])):\n",
        "            list_para = train_data[\"data\"][id][\"paragraphs\"]\n",
        "            for para in list_para:\n",
        "                context = para[\"context\"]\n",
        "                qas = para[\"qas\"]\n",
        "                q = []\n",
        "                a = []\n",
        "                for question in qas:\n",
        "                    questions = question[\"question\"]\n",
        "                    answer_text = question[\"answers\"][0][\"text\"]\n",
        "                    answer_start = question[\"answers\"][0][\"answer_start\"]\n",
        "                    \n",
        "                    start = 0\n",
        "                    for c in range(answer_start):\n",
        "                      if(context[c] == ' '):\n",
        "                        start += 1\n",
        "                    answer_length = len(list(answer_text.split()))\n",
        "                    if(answer_length == 1):\n",
        "                      context_data.append(context)\n",
        "                      question_data.append(questions)\n",
        "                      answer_start_data.append(start)\n",
        "                      answer_length_data.append(answer_length)\n",
        "                    q.append(questions)\n",
        "                    a.append(answer_text)\n",
        "\n",
        "            context_list.append(context)\n",
        "            question_list.append(q)\n",
        "            answer_text_list.append(a)\n",
        "        print(len(context_list))\n",
        "        print(len(question_list))\n",
        "        print(len(answer_text_list))\n",
        "        # make a list for all and then put it    \n",
        "        dict = {'context': context_list , 'question': question_list , 'answer': answer_text_list}\n",
        "        dict2 = {'context' : context_data, 'output' : question_data, 'answer_start':answer_start_data, 'answer_length':answer_length_data}\n",
        "        df2 = pd.DataFrame.from_dict(dict2)\n",
        "        df2.to_csv(\"new1.csv\")\n",
        "        new = pd.DataFrame.from_dict(dict)\n",
        "        new.to_csv(\"output.csv\")\n",
        "\n",
        "    def load_data(self, filename , directory):\n",
        "        try:\n",
        "            json_path = os.path.join(directory, filename)\n",
        "            print(json_path)\n",
        "            file = open(json_path)\n",
        "            data = json.load(file)\n",
        "            file.close()\n",
        "            return data\n",
        "        except:\n",
        "            print(\"unable to load a json file\")\n",
        "    def download_dataset(self, url , filename, directory):\n",
        "        try:\n",
        "            save_path = os.path.join(directory,filename)\n",
        "            \n",
        "            # if not present then download\n",
        "            if not os.path.exists(save_path):\n",
        "                # if folder doesnt exist\n",
        "\n",
        "                url = os.path.join(url,filename)\n",
        "                urllib.request.urlretrieve(url,save_path)\n",
        "\n",
        "        except:\n",
        "            print(\"some error occured\")\n",
        "\n",
        "    \n",
        "preprocessing = Preprocessing()\n",
        "preprocessing.start()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('new1.csv')"
      ],
      "metadata": {
        "id": "n4EltXefSLAf"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56t6oJuYTPaN",
        "outputId": "2be1d1c1-a051-4e9f-f4b3-b988178f2ce3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 30267 entries, 0 to 30266\n",
            "Data columns (total 5 columns):\n",
            " #   Column         Non-Null Count  Dtype \n",
            "---  ------         --------------  ----- \n",
            " 0   Unnamed: 0     30267 non-null  int64 \n",
            " 1   context        30267 non-null  object\n",
            " 2   output         30267 non-null  object\n",
            " 3   answer_start   30267 non-null  int64 \n",
            " 4   answer_length  30267 non-null  int64 \n",
            "dtypes: int64(3), object(2)\n",
            "memory usage: 1.2+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "9YswVxPiSRqr"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
        "tokenizer.fit_on_texts(np.array(df.context[0:10000]))\n",
        "tokenized_input = tokenizer.texts_to_sequences(np.array(df.context[0:10000]))\n",
        "\n",
        "max_input_len = max([len(i) for i in tokenized_input])\n",
        "\n",
        "padded_lines = tf.keras.utils.pad_sequences(tokenized_input, padding='post')\n",
        "\n",
        "num_input_tokens = len(tokenizer.word_index) + 1\n",
        "\n",
        "input_word_dict = tokenizer.word_index"
      ],
      "metadata": {
        "id": "kAF9BUrnSUxG"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer2 = tf.keras.preprocessing.text.Tokenizer()\n",
        "tokenizer2.fit_on_texts(df.output[0:10000])\n",
        "tokenized_output = tokenizer2.texts_to_sequences(np.array(df.output[0:10000]))\n",
        "\n",
        "padded_output_lines = tf.keras.utils.pad_sequences(tokenized_output, padding='post')\n",
        "\n",
        "output_word_dict = tokenizer.word_index\n",
        "\n",
        "max_output_len = max([len(i) for i in tokenized_output])\n",
        "\n",
        "num_out_tokens = len(output_word_dict) + 1"
      ],
      "metadata": {
        "id": "rdVejDPlSW8N"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(max_input_len, max_output_len, num_input_tokens, num_out_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QHafOunDUkMo",
        "outputId": "bf5c4c5a-0602-4990-8f1b-2d52e3974d81"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "518 31 33879 33879\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "answer_start = np.array(df.answer_start[0:10000], dtype=int)\n",
        "answer_length = np.array(df.answer_length[0:10000], dtype=int)\n",
        "onehot_start = tf.keras.utils.to_categorical(answer_start, max_input_len)\n",
        "onehot_length = tf.keras.utils.to_categorical(answer_length, max_input_len)"
      ],
      "metadata": {
        "id": "qcXQmhE2U-qn"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoder_target_data = np.array(onehot_start)"
      ],
      "metadata": {
        "id": "HsvFm0-DUm7L"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoder_target_data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tVerhkyyVhWG",
        "outputId": "63dada1c-abea-4dde-857d-41cdbe575977"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 518)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dimensionality = 256\n",
        "from tensorflow.keras.layers import Input, LSTM\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras import preprocessing, utils, layers, activations, models"
      ],
      "metadata": {
        "id": "5LDTa2yqViXz"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_inputs = Input(shape=(None, ))\n",
        "encoder_embedding = Embedding(num_input_tokens, dimensionality, mask_zero=True)(encoder_inputs)\n",
        "encoder_outputs, state_h, state_c = LSTM(dimensionality, return_state=True, recurrent_dropout=0.2,\n",
        "                                         dropout=0.2)(encoder_embedding)\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "decoder_inputs = Input(shape=(None,))\n",
        "decoder_embedding = Embedding(num_out_tokens, dimensionality, mask_zero=True)(decoder_inputs)\n",
        "decoder_lstm = LSTM(dimensionality, return_state=True, recurrent_dropout=0.2, dropout=0.2)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
        "decoder_dense = tf.keras.layers.Dense(dimensionality, activation=tf.keras.activations.relu)\n",
        "output = decoder_dense(decoder_outputs)\n",
        "decoder_dense1 = tf.keras.layers.Dense(max_input_len, activation=tf.keras.activations.softmax)\n",
        "output1 = decoder_dense1(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XfX6wjF3byji",
        "outputId": "018e44de-f92c-49b0-ef39-de4c9c82cc44"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.Model([encoder_inputs, decoder_inputs], output1)\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "\n",
        "history = model.fit([padded_lines, padded_output_lines],\n",
        "                    np.array(onehot_start),\n",
        "                    validation_split=0.33,\n",
        "                    batch_size=32,\n",
        "                    epochs=10,\n",
        "                    shuffle=True)\n",
        "\n",
        "model.save('model.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qv5Hbsp5ckPH",
        "outputId": "ca760dce-7f01-4df5-b606-5aa43c3d6ad2"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " embedding (Embedding)          (None, None, 256)    8673024     ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " embedding_1 (Embedding)        (None, None, 256)    8673024     ['input_2[0][0]']                \n",
            "                                                                                                  \n",
            " lstm (LSTM)                    [(None, 256),        525312      ['embedding[0][0]']              \n",
            "                                 (None, 256),                                                     \n",
            "                                 (None, 256)]                                                     \n",
            "                                                                                                  \n",
            " lstm_1 (LSTM)                  [(None, 256),        525312      ['embedding_1[0][0]',            \n",
            "                                 (None, 256),                     'lstm[0][1]',                   \n",
            "                                 (None, 256)]                     'lstm[0][2]']                   \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 256)          65792       ['lstm_1[0][0]']                 \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 518)          133126      ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 18,595,590\n",
            "Trainable params: 18,595,590\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/10\n",
            "210/210 [==============================] - 590s 3s/step - loss: 4.9550 - accuracy: 0.0248 - val_loss: 4.8126 - val_accuracy: 0.0264\n",
            "Epoch 2/10\n",
            "210/210 [==============================] - 585s 3s/step - loss: 4.7460 - accuracy: 0.0278 - val_loss: 4.7776 - val_accuracy: 0.0279\n",
            "Epoch 3/10\n",
            "210/210 [==============================] - 565s 3s/step - loss: 4.5652 - accuracy: 0.0373 - val_loss: 4.8754 - val_accuracy: 0.0212\n",
            "Epoch 4/10\n",
            "210/210 [==============================] - 561s 3s/step - loss: 4.0503 - accuracy: 0.0984 - val_loss: 5.1713 - val_accuracy: 0.0139\n",
            "Epoch 5/10\n",
            "210/210 [==============================] - 563s 3s/step - loss: 3.1673 - accuracy: 0.2739 - val_loss: 6.0652 - val_accuracy: 0.0112\n",
            "Epoch 6/10\n",
            "210/210 [==============================] - 560s 3s/step - loss: 2.2250 - accuracy: 0.4787 - val_loss: 7.5085 - val_accuracy: 0.0130\n",
            "Epoch 7/10\n",
            "210/210 [==============================] - 561s 3s/step - loss: 1.4778 - accuracy: 0.6468 - val_loss: 8.6721 - val_accuracy: 0.0127\n",
            "Epoch 8/10\n",
            "210/210 [==============================] - 566s 3s/step - loss: 0.9601 - accuracy: 0.7612 - val_loss: 9.8761 - val_accuracy: 0.0121\n",
            "Epoch 9/10\n",
            "210/210 [==============================] - 562s 3s/step - loss: 0.5877 - accuracy: 0.8583 - val_loss: 11.3368 - val_accuracy: 0.0109\n",
            "Epoch 10/10\n",
            "210/210 [==============================] - 557s 3s/step - loss: 0.3494 - accuracy: 0.9149 - val_loss: 12.6534 - val_accuracy: 0.0130\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('model.h5') "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "NEGj75GVc2Qc",
        "outputId": "682149c5-fd29-40b9-c334-3ba0a394e3a7"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_0e826979-36d7-484f-8d04-1ec82c0dd6bf\", \"model.h5\", 223212280)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8tu3K2N0504C"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}